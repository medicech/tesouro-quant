import sys
import os
import requests
import pandas as pd
from bs4 import BeautifulSoup
from datetime import datetime
import re
from pathlib import Path

# --- CONFIGURA√á√ÉO DE CAMINHOS (CR√çTICO PARA NUVEM) ---
current_dir = os.path.dirname(os.path.abspath(__file__))
root_dir = os.path.dirname(current_dir)
sys.path.append(os.path.join(root_dir, "src"))

try:
    from core.config import DATA_DIR
    PROCESSED_DIR = DATA_DIR / "processed"
except ImportError:
    DATA_DIR = Path(root_dir) / "data"
    PROCESSED_DIR = DATA_DIR / "processed"

# --- CONFIGURA√á√ïES ---
URL_ALVO = "https://investidor10.com.br/tesouro-direto/"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36"
}

# --- FUN√á√ïES DE LIMPEZA ---
def clean_money(text):
    """Transforma 'R$ 1.234,56' em float 1234.56"""
    if not text: return 0.0
    clean = text.replace('R$', '').replace('.', '').replace(',', '.').strip()
    try: return float(clean)
    except: return 0.0

def clean_rate(text):
    """Transforma 'IPCA + 6,50%' em float 6.50"""
    if not text: return 0.0
    match = re.search(r'([\d,]+)%', text)
    if match:
        clean = match.group(1).replace(',', '.')
        return float(clean)
    clean = text.replace('%', '').replace(',', '.').strip()
    try: return float(clean)
    except: return 0.0

def main():
    print("üïµÔ∏è‚Äç‚ôÇÔ∏è Iniciando Scraping do Investidor10 (Produ√ß√£o)...")
    
    # Garante que a pasta existe
    os.makedirs(PROCESSED_DIR, exist_ok=True)
    
    try:
        # 1. Requisi√ß√£o
        response = requests.get(URL_ALVO, headers=HEADERS, timeout=20)
        response.raise_for_status()
        
        # 2. Parse HTML
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Tenta achar a tabela (id com erro de digita√ß√£o do site ou classe gen√©rica)
        table = soup.find('table', {'id': 'rankigns'})
        if not table:
            table = soup.find('table', {'class': 'table'})
            
        if not table:
            print("‚ùå Erro: Tabela n√£o encontrada no HTML.")
            sys.exit(1) # For√ßa erro no Streamlit

        rows = table.find('tbody').find_all('tr')
        print(f"‚úÖ Encontradas {len(rows)} linhas.")
        
        dados_processados = []
        
        for tr in rows:
            cols = tr.find_all('td')
            if len(cols) < 6: continue
            
            # Mapeamento Investidor10:
            # 1: Nome | 2: Rentabilidade | 3: M√≠nimo | 4: Pre√ßo | 5: Vencimento
            nome = cols[1].get_text().strip()
            rentabilidade = cols[2].get_text().strip()
            minimo = cols[3].get_text().strip()
            preco = cols[4].get_text().strip()
            vencimento = cols[5].get_text().strip()
            
            if not nome or "T√≠tulo" in nome: continue
            
            # Tratamento de Data
            try:
                dt_venc = pd.to_datetime(vencimento, dayfirst=True)
            except:
                continue

            # Classifica√ß√£o do Indexador (Essencial para os filtros do site)
            nome_upper = nome.upper()
            if "IPCA" in nome_upper or "RENDA+" in nome_upper or "EDUCA+" in nome_upper:
                indexador = "IPCA"
            elif "SELIC" in nome_upper:
                indexador = "SELIC"
            elif "PREFIXADO" in nome_upper:
                indexador = "PREFIXADO"
            else:
                indexador = "OUTROS"

            dados_processados.append({
                "tipo_titulo": nome,
                "vencimento": dt_venc,
                "data_base": datetime.now(),
                "taxa_compra": clean_rate(rentabilidade),
                "pu_compra": clean_money(preco),
                "minimo_compra": clean_money(minimo),
                "taxa_venda": 0.0, # Site n√£o fornece f√°cil
                "pu_venda": 0.0,   # Site n√£o fornece f√°cil
                "indexador": indexador,
                "ano_vencimento": dt_venc.year
            })

        if not dados_processados:
            print("‚ùå Erro: Nenhum dado extra√≠do.")
            sys.exit(1)

        # 3. Limpeza e Salvamento
        # Remove arquivos antigos para o site n√£o ler dado velho
        for f in PROCESSED_DIR.glob("tesouro_catalogo_*.parquet"):
            try: os.remove(f)
            except: pass

        df = pd.DataFrame(dados_processados)
        hoje_iso = datetime.now().date().isoformat()
        arquivo_saida = PROCESSED_DIR / f"tesouro_catalogo_{hoje_iso}.parquet"
        
        df.to_parquet(arquivo_saida, index=False)
        print(f"üíæ SUCESSO! Salvo em: {arquivo_saida}")
        
        # Preview no Log do Streamlit
        print("üìä Amostra:")
        print(df[['tipo_titulo', 'taxa_compra', 'pu_compra']].head(3))

    except Exception as e:
        print(f"‚ùå Erro Cr√≠tico: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
